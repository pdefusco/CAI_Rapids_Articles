{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f168aad1-1adf-472e-809d-8fe7c744b689",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, warnings, sys, logging\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "# use the GPU-native implementation\n",
    "from spark_rapids_ml.classification import RandomForestClassifier\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import pprint\n",
    "\n",
    "# Force-clear any hanging Py4J connections\n",
    "from py4j.java_gateway import java_import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d9ba8cd-19bd-4080-a819-a3a7e169929c",
   "metadata": {},
   "outputs": [],
   "source": [
    "USERNAME = os.environ[\"PROJECT_OWNER\"]\n",
    "DBNAME = \"DEMO_\"+USERNAME\n",
    "CONNECTION_NAME = \"go01-aw-dl\"\n",
    "STORAGE =  os.environ[\"DATA_STORAGE\"] \n",
    "DATE = date.today()\n",
    "\n",
    "RAPIDS_JAR = \"/home/cdsw/rapids-4-spark_2.12-25.10.0.jar\"\n",
    "\n",
    "\n",
    "LOCAL_PACKAGES = \"/home/cdsw/.local/lib/python3.10/site-packages\"\n",
    "# This is where the specific CUDA 12 NVRTC library lives\n",
    "NVRTC_LIB_PATH = f\"{LOCAL_PACKAGES}/nvidia/cuda_nvrtc/lib\"\n",
    "WRITABLE_CACHE_DIR = \"/tmp/cupy_cache\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "984a95b0-b6e6-4410-9c00-24aefaa161ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting spark.hadoop.yarn.resourcemanager.principal to jprosser\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Java Context Object: org.apache.spark.api.java.JavaSparkContext@7a7278f3'\n",
      "'Master: k8s://https://172.20.0.1:443'\n",
      "'Spark User: jprosser'\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark-Rapids-32GB-Final\") \\\n",
    "    .config(\"spark.executor.resource.gpu.vendor\", \"nvidia.com\") \\\n",
    "    .config(\"spark.executor.resource.gpu.discoveryScript\", \"/home/cdsw/spark-rapids-ml/getGpusResources.sh\") \\\n",
    "    .config(\"spark.executorEnv.LD_LIBRARY_PATH\", f\"{NVRTC_LIB_PATH}:{os.environ.get('LD_LIBRARY_PATH', '')}\") \\\n",
    "    .config(\"spark.executorEnv.PYTHONPATH\", LOCAL_PACKAGES) \\\n",
    "    .config(\"spark.executorEnv.CUPY_CACHE_DIR\", WRITABLE_CACHE_DIR) \\\n",
    "    .config(\"spark.driverEnv.CUPY_CACHE_DIR\", WRITABLE_CACHE_DIR) \\\n",
    "    .config(\"spark.driver.memory\", \"12g\") \\\n",
    "    .config(\"spark.driver.extraJavaOptions\", f\"-Djava.library.path={NVRTC_LIB_PATH}\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"4g\") \\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"false\") \\\n",
    "    .config(\"spark.executor.cores\", 3) \\\n",
    "    .config(\"spark.executor.instances\", 1) \\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"60s\") \\\n",
    "    .config(\"spark.executor.memory\", \"10g\") \\\n",
    "    .config(\"spark.executor.resource.gpu.amount\", 1) \\\n",
    "    .config(\"spark.executor.memoryOverhead\", \"10g\") \\\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", -1) \\\n",
    "    .config(\"spark.sql.broadcastTimeout\", \"1200\") \\\n",
    "    .config(\"spark.sql.cache.serializer\", \"com.nvidia.spark.ParquetCachedBatchSerializer\") \\\n",
    "    .config('spark.sql.shuffle.partitions', '200') \\\n",
    "    .config(\"spark.network.timeout\", \"800s\") \\\n",
    "    .config(\"spark.rapids.sql.enabled\", \"true\") \\\n",
    "    .config(\"spark.rapids.shims-provider-override\", \"com.nvidia.spark.rapids.shims.spark351.SparkShimServiceProvider\") \\\n",
    "    .config(\"spark.rapids.memory.pinnedPool.size\", \"4g\") \\\n",
    "    .config(\"spark.task.resource.gpu.amount\", 0.33) \\\n",
    "    .config(\"spark.jars\", RAPIDS_JAR) \\\n",
    "    .config(\"spark.kerberos.access.hadoopFileSystems\", \"s3a://go01-demo/user/jprosser/spark-rapids-ml/\") \\\n",
    "    .config(\"spark.plugins\", \"com.nvidia.spark.SQLPlugin\") \\\n",
    "    .config(\"spark.shuffle.service.enabled\", \"false\") \\\n",
    "    .config('spark.shuffle.file.buffer', '64k') \\\n",
    "    .config('spark.shuffle.spill.compress', 'true') \\\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"s3a://go01-demo/\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "# View the underlying Java Spark Context\n",
    "pprint.pprint(f\"Java Context Object: {spark.sparkContext._jsc}\")\n",
    "\n",
    "# View the Spark Master (in CML, this usually points to the local container or YARN)\n",
    "pprint.pprint(f\"Master: {spark.sparkContext.master}\")\n",
    "\n",
    "# View the User running the session\n",
    "pprint.pprint(f\"Spark User: {spark.sparkContext.sparkUser()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19fde3c3-fa2e-4bd1-8a3e-598950130e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable CollectLimit so that large datasets are collected on the GPU.\n",
    "# Not worth it for small datasets\n",
    "spark.conf.set(\"spark.rapids.sql.exec.CollectLimitExec\", \"true\")\n",
    "\n",
    "# Enabled to let the GPU to handle the random sampling of rows for large datasets\n",
    "spark.conf.set(\"spark.rapids.sql.exec.SampleExec\", \"true\")\n",
    "\n",
    "# Enabled to let allow more time for large broadcast joins\n",
    "spark.conf.set(\"spark.sql.broadcastTimeout\", \"1200\") # Increase to 20 mins\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "#spark.conf.set(\"spark.rapids.sql.explain\", \"ALL\")\n",
    "spark.conf.set(\"spark.rapids.sql.explain\", \"NOT_ON_GPU\") # Only log when/why the GPU was not selected\n",
    "spark.conf.set(\"spark.rapids.sql.variable.float.allow\", \"true\") # Allow float math\n",
    "\n",
    "# Allow the GPU to cast instead of pushing back to CPU just for cast\n",
    "spark.conf.set(\"spark.rapids.sql.castFloatToDouble.enabled\", \"true\") \n",
    "spark.conf.set(\"spark.rapids.sql.format.parquet.enabled\", \"true\")\n",
    "\n",
    "# Turning off Adaptive Query Execution (AQE) makes the entire SQL plan use the GPU\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d7b7adc-c11f-439f-9420-f5d7689747f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Driver Version: 12080\n",
      "Device Count: 1\n",
      "Dynamic Allocation: false\n",
      "Executor Instances: 1\n",
      "Dynamic Allocation Enabled: false\n"
     ]
    }
   ],
   "source": [
    "# Test if the JVM can actually talk to the CUDA driver\n",
    "cuda_manager = spark._jvm.ai.rapids.cudf.Cuda\n",
    "print(f\"CUDA Driver Version: {cuda_manager.getDriverVersion()}\")\n",
    "print(f\"Device Count: {cuda_manager.getDeviceCount()}\")\n",
    "print(f\"Dynamic Allocation: {spark.conf.get('spark.dynamicAllocation.enabled')}\")\n",
    "print(f\"Executor Instances: {spark.conf.get('spark.executor.instances')}\")\n",
    "print(f\"Dynamic Allocation Enabled: {spark.conf.get('spark.dynamicAllocation.enabled')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01e89282-3aed-4d6a-94a8-d4f9b455af4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debug enabled for RAPIDS: True\n"
     ]
    }
   ],
   "source": [
    "# Test acess to the SQLPlugin\n",
    "sql_plugin = spark._jvm.com.nvidia.spark.SQLPlugin()\n",
    "\n",
    "driver_comp = sql_plugin.driverPlugin()\n",
    "\n",
    "\n",
    "log_manager = spark._jvm.org.apache.log4j.LogManager\n",
    "level_debug = spark._jvm.org.apache.log4j.Level.DEBUG\n",
    "\n",
    "logger = driver_comp.log()\n",
    "log_manager.getLogger(\"com.nvidia.spark.rapids\").setLevel(level_debug)\n",
    "\n",
    "print(f\"Debug enabled for RAPIDS: {driver_comp.isTraceEnabled() or True}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15e2175a-9033-4975-86f3-0e190d628a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/02 13:27:50 WARN  client.HiveClientImpl: [Thread-6]: Detected HiveConf hive.execution.engine is 'tez' and will be reset to 'mr' to disable useless hive logic\n",
      "Hive Session ID = 957c8015-78b1-4e53-b297-fc5206e0537b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: 15\n",
      "Schema: StructType([StructField('age', FloatType(), True), StructField('credit_card_balance', FloatType(), True), StructField('bank_account_balance', FloatType(), True), StructField('mortgage_balance', FloatType(), True), StructField('sec_bank_account_balance', FloatType(), True), StructField('savings_account_balance', FloatType(), True), StructField('sec_savings_account_balance', FloatType(), True), StructField('total_est_nworth', FloatType(), True), StructField('primary_loan_balance', FloatType(), True), StructField('secondary_loan_balance', FloatType(), True), StructField('uni_loan_balance', FloatType(), True), StructField('longitude', FloatType(), True), StructField('latitude', FloatType(), True), StructField('transaction_amount', FloatType(), True), StructField('fraud_trx', IntegerType(), True)])\n",
      "== Physical Plan ==\n",
      "GpuColumnarToRow (6)\n",
      "+- GpuGlobalLimit (5)\n",
      "   +- GpuShuffleCoalesce (4)\n",
      "      +- GpuColumnarExchange (3)\n",
      "         +- GpuLocalLimit (2)\n",
      "            +- GpuScan parquet spark_catalog.default.datalaketable (1)\n",
      "\n",
      "\n",
      "(1) GpuScan parquet spark_catalog.default.datalaketable\n",
      "Output [15]: [age#0, credit_card_balance#1, bank_account_balance#2, mortgage_balance#3, sec_bank_account_balance#4, savings_account_balance#5, sec_savings_account_balance#6, total_est_nworth#7, primary_loan_balance#8, secondary_loan_balance#9, uni_loan_balance#10, longitude#11, latitude#12, transaction_amount#13, fraud_trx#14]\n",
      "Batched: true\n",
      "Eager_IO_Prefetch: false\n",
      "Location: InMemoryFileIndex [s3a://go01-demo/warehouse/tablespace/external/hive/customer_data HB01/warehouse/tablespace/external/hive/datalaketable]\n",
      "ReadSchema: struct<age:float,credit_card_balance:float,bank_account_balance:float,mortgage_balance:float,sec_bank_account_balance:float,savings_account_balance:float,sec_savings_account_balance:float,total_est_nworth:float,primary_loan_balance:float,secondary_loan_balance:float,uni_loan_balance:float,longitude:float,latitude:float,transaction_amount:float,fraud_trx:int>\n",
      "\n",
      "(2) GpuLocalLimit\n",
      "Input [15]: [age#0, credit_card_balance#1, bank_account_balance#2, mortgage_balance#3, sec_bank_account_balance#4, savings_account_balance#5, sec_savings_account_balance#6, total_est_nworth#7, primary_loan_balance#8, secondary_loan_balance#9, uni_loan_balance#10, longitude#11, latitude#12, transaction_amount#13, fraud_trx#14]\n",
      "Arguments: 5\n",
      "\n",
      "(3) GpuColumnarExchange\n",
      "Input [15]: [age#0, credit_card_balance#1, bank_account_balance#2, mortgage_balance#3, sec_bank_account_balance#4, savings_account_balance#5, sec_savings_account_balance#6, total_est_nworth#7, primary_loan_balance#8, secondary_loan_balance#9, uni_loan_balance#10, longitude#11, latitude#12, transaction_amount#13, fraud_trx#14]\n",
      "Arguments: gpusinglepartitioning$(), ENSURE_REQUIREMENTS, [plan_id=10]\n",
      "\n",
      "(4) GpuShuffleCoalesce\n",
      "Input [15]: [age#0, credit_card_balance#1, bank_account_balance#2, mortgage_balance#3, sec_bank_account_balance#4, savings_account_balance#5, sec_savings_account_balance#6, total_est_nworth#7, primary_loan_balance#8, secondary_loan_balance#9, uni_loan_balance#10, longitude#11, latitude#12, transaction_amount#13, fraud_trx#14]\n",
      "Arguments: 1073741824\n",
      "\n",
      "(5) GpuGlobalLimit\n",
      "Input [15]: [age#0, credit_card_balance#1, bank_account_balance#2, mortgage_balance#3, sec_bank_account_balance#4, savings_account_balance#5, sec_savings_account_balance#6, total_est_nworth#7, primary_loan_balance#8, secondary_loan_balance#9, uni_loan_balance#10, longitude#11, latitude#12, transaction_amount#13, fraud_trx#14]\n",
      "Arguments: 5, 0\n",
      "\n",
      "(6) GpuColumnarToRow\n",
      "Input [15]: [age#0, credit_card_balance#1, bank_account_balance#2, mortgage_balance#3, sec_bank_account_balance#4, savings_account_balance#5, sec_savings_account_balance#6, total_est_nworth#7, primary_loan_balance#8, secondary_loan_balance#9, uni_loan_balance#10, longitude#11, latitude#12, transaction_amount#13, fraud_trx#14]\n",
      "Arguments: false\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Do a test connection to a DB and show the GPU in action\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "df = spark.read.table(\"DataLakeTable\")\n",
    "print(f\"Columns: {len(df.columns)}\")\n",
    "print(f\"Schema: {df.schema}\")\n",
    "# Look for 'Gpu' operators in the output\n",
    "df.limit(5).explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9316aa9e-0b91-4cc1-9489-33e6066f5e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Experimental Methods: org.apache.spark.sql.ExperimentalMethods@1c310484'\n"
     ]
    }
   ],
   "source": [
    "# Access the Java 'SessionState' through the back door\n",
    "jvm_session_state = spark._jsparkSession.sessionState()\n",
    "\n",
    "# Check if the Catalyst Optimizer is using the RAPIDS extensions\n",
    "pprint.pprint(f\"Experimental Methods: {jvm_session_state.experimentalMethods()}\")\n",
    "\n",
    "# Access the experimental methods via the JVM bridge\n",
    "experimental = spark._jsparkSession.sessionState().experimentalMethods()\n",
    "\n",
    "# Enable SampleExec (another commonly disabled-by-default op)\n",
    "spark.conf.set(\"spark.rapids.sql.exec.SampleExec\", \"true\")\n",
    "spark.conf.set(\"spark.sql.broadcastTimeout\", \"1200\") # Increase to 20 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1f1344f-9b70-41de-a316-e99b226e349a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/02 13:28:24 WARN  scheduler.TaskSchedulerImpl: [task-starvation-timer]: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "26/02/02 13:28:39 WARN  scheduler.TaskSchedulerImpl: [task-starvation-timer]: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "26/02/02 13:28:54 WARN  scheduler.TaskSchedulerImpl: [task-starvation-timer]: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+\n",
      "|    category|           avg_val|\n",
      "+------------+------------------+\n",
      "|Category_493| 51.01314531748956|\n",
      "|Category_425| 50.92732004901775|\n",
      "|Category_312|50.856966235932894|\n",
      "|Category_388| 50.82908321026169|\n",
      "|Category_335| 50.77918679311496|\n",
      "|Category_414| 50.77466769977464|\n",
      "|Category_758| 50.76249040325846|\n",
      "|Category_688|50.729797095056455|\n",
      "|Category_421| 50.71255550280795|\n",
      "|Category_754| 50.71146305713495|\n",
      "+------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "== Physical Plan ==\n",
      "GpuColumnarToRow (18)\n",
      "+- GpuSort (17)\n",
      "   +- GpuShuffleCoalesce (16)\n",
      "      +- GpuColumnarExchange (15)\n",
      "         +- GpuHashAggregate (14)\n",
      "            +- GpuShuffleCoalesce (13)\n",
      "               +- GpuColumnarExchange (12)\n",
      "                  +- GpuHashAggregate (11)\n",
      "                     +- GpuProject (10)\n",
      "                        +- GpuShuffledSymmetricHashJoin (9)\n",
      "                           :- GpuColumnarExchange (5)\n",
      "                           :  +- GpuCoalesceBatches (4)\n",
      "                           :     +- GpuFilter (3)\n",
      "                           :        +- GpuProject (2)\n",
      "                           :           +- GpuRange (1)\n",
      "                           +- GpuColumnarExchange (8)\n",
      "                              +- GpuProject (7)\n",
      "                                 +- GpuRange (6)\n",
      "\n",
      "\n",
      "(1) GpuRange\n",
      "Output [1]: [id#30L]\n",
      "Arguments: 0, 10000000, 1, 2, [id#30L], 1073741824\n",
      "\n",
      "(2) GpuProject\n",
      "Input [1]: [id#30L]\n",
      "Arguments: [(id#30L % 1000) AS join_key#32L, (gpurand(42, false) * 100.0) AS data_value#35], true\n",
      "\n",
      "(3) GpuFilter\n",
      "Input [2]: [join_key#32L, data_value#35]\n",
      "Arguments: gpuisnotnull(join_key#32L)\n",
      "\n",
      "(4) GpuCoalesceBatches\n",
      "Input [2]: [join_key#32L, data_value#35]\n",
      "Arguments: targetsize(1073741824)\n",
      "\n",
      "(5) GpuColumnarExchange\n",
      "Input [2]: [join_key#32L, data_value#35]\n",
      "Arguments: gpuhashpartitioning(join_key#32L, 200, Murmur3Mode), ENSURE_REQUIREMENTS, [plan_id=284]\n",
      "\n",
      "(6) GpuRange\n",
      "Output [1]: [id#39L]\n",
      "Arguments: 0, 1000, 1, 2, [id#39L], 1073741824\n",
      "\n",
      "(7) GpuProject\n",
      "Input [1]: [id#39L]\n",
      "Arguments: [id#39L AS join_key#41L, gpuconcat(Category_, cast(id#39L as string)) AS category#43], true\n",
      "\n",
      "(8) GpuColumnarExchange\n",
      "Input [2]: [join_key#41L, category#43]\n",
      "Arguments: gpuhashpartitioning(join_key#41L, 200, Murmur3Mode), ENSURE_REQUIREMENTS, [plan_id=257]\n",
      "\n",
      "(9) GpuShuffledSymmetricHashJoin\n",
      "Left output [2]: [join_key#32L, data_value#35]\n",
      "Right output [2]: [join_key#41L, category#43]\n",
      "Arguments: Inner, [join_key#32L], [join_key#41L], false, 1073741824, 1.0, CoalesceReadOption(true,Never,None,false), false\n",
      "\n",
      "(10) GpuProject\n",
      "Input [4]: [join_key#32L, data_value#35, join_key#41L, category#43]\n",
      "Arguments: [data_value#35, category#43], true\n",
      "\n",
      "(11) GpuHashAggregate\n",
      "Input [2]: [data_value#35, category#43]\n",
      "Keys [1]: [category#43]\n",
      "Functions [1]: [partial_avg(data_value#35, DoubleType, false)]\n",
      "Aggregate Attributes [2]: [sum#64, count#65L]\n",
      "Results [3]: [category#43, sum#66, count#67L]\n",
      "Lore: \n",
      "\n",
      "(12) GpuColumnarExchange\n",
      "Input [3]: [category#43, sum#66, count#67L]\n",
      "Arguments: gpuhashpartitioning(category#43, 200, Murmur3Mode), ENSURE_REQUIREMENTS, [plan_id=311]\n",
      "\n",
      "(13) GpuShuffleCoalesce\n",
      "Input [3]: [category#43, sum#66, count#67L]\n",
      "Arguments: 1073741824\n",
      "\n",
      "(14) GpuHashAggregate\n",
      "Input [3]: [category#43, sum#66, count#67L]\n",
      "Keys [1]: [category#43]\n",
      "Functions [1]: [avg(data_value#35, DoubleType, false)]\n",
      "Aggregate Attributes [1]: [avg(data_value#35)#54]\n",
      "Results [2]: [category#43, avg(data_value#35)#54 AS avg_val#55]\n",
      "Lore: \n",
      "\n",
      "(15) GpuColumnarExchange\n",
      "Input [2]: [category#43, avg_val#55]\n",
      "Arguments: gpurangepartitioning(avg_val#55 DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [plan_id=314]\n",
      "\n",
      "(16) GpuShuffleCoalesce\n",
      "Input [2]: [category#43, avg_val#55]\n",
      "Arguments: 1073741824\n",
      "\n",
      "(17) GpuSort\n",
      "Input [2]: [category#43, avg_val#55]\n",
      "Arguments: [avg_val#55 DESC NULLS LAST], true, com.nvidia.spark.rapids.OutOfCoreSort$@5b330fe3\n",
      "\n",
      "(18) GpuColumnarToRow\n",
      "Input [2]: [category#43, avg_val#55]\n",
      "Arguments: false\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create two large-ish dataframes and join them\n",
    "# This creates 10 million rows to give the GPU something to chew on\n",
    "left_df = spark.range(0, 10000000) \\\n",
    "    .withColumn(\"join_key\", F.col(\"id\") % 1000) \\\n",
    "    .withColumn(\"data_value\", F.rand(seed=42) * 100)\n",
    "\n",
    "right_df = spark.range(0, 1000) \\\n",
    "    .withColumnRenamed(\"id\", \"join_key\") \\\n",
    "    .withColumn(\"category\", F.concat(F.lit(\"Category_\"), F.col(\"join_key\")))\n",
    "\n",
    "# We use a inner join here on 'join_key'\n",
    "# GPUs prefer HASH so we give it a hint\n",
    "joined_df = left_df.hint(\"SHUFFLE_HASH\").join(right_df, on=\"join_key\", how=\"inner\")\n",
    "\n",
    "# Perform an Aggregation to trigger a Shuffle\n",
    "final_result = joined_df.groupBy(\"category\") \\\n",
    "    .agg(F.avg(\"data_value\").alias(\"avg_val\")) \\\n",
    "    .orderBy(F.desc(\"avg_val\"))\n",
    "\n",
    "\n",
    "final_result.show(10)\n",
    "\n",
    "# Check the Physical Plan\n",
    "# Look for 'GpuHashJoin' and 'GpuColumnarExchange'\n",
    "final_result.explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4acd08e-0a41-4352-95ea-bf6b0c2193e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform data into a single vector column \n",
    "feature_cols = [\"age\", \"credit_card_balance\", \"bank_account_balance\", \"mortgage_balance\", \"sec_bank_account_balance\", \"savings_account_balance\",\n",
    "                    \"sec_savings_account_balance\", \"total_est_nworth\", \"primary_loan_balance\", \"secondary_loan_balance\", \"uni_loan_balance\",\n",
    "                    \"longitude\", \"latitude\", \"transaction_amount\"]\n",
    "\n",
    "# Avoid VectorAssembler as it creates VectorUDT data types that are not GPU Friendly\n",
    "#assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "#df_assembled = assembler.transform(df)\n",
    "# Split data into training and test sets\n",
    "#(training_data, test_data) = df_assembled.randomSplit([0.8, 0.2], seed=1234)\n",
    "\n",
    "(training_data, test_data) = df.randomSplit([0.8, 0.2], seed=1234)\n",
    "\n",
    "\n",
    "# Use spark_rapids_ml.classification.RandomForestClassifier\n",
    "\n",
    "# Import from spark_rapids_ml to use the GPU-native implementation\n",
    "from spark_rapids_ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Define the RAPIDS-native classifier\n",
    "# As noted above, by using 'featuresCols' (list of strings), we avoid VectorAssembler \n",
    "# \n",
    "rf_classifier = RandomForestClassifier(\n",
    "    labelCol=\"fraud_trx\", \n",
    "    featuresCols=feature_cols, \n",
    "    numTrees=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e0d1ec5-4f73-4076-8699-1f3c5076eb94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-02 13:29:18,431 - spark_rapids_ml.classification.RandomForestClassifier - INFO - Training spark-rapids-ml with 1 worker(s) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Spark RAPIDS ML model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-02 13:29:19,132 - spark_rapids_ml.classification.RandomForestClassifier - INFO - Training tasks require the resource(cores=3, gpu=1.0)\n",
      "2026-02-02 13:30:22,152 - spark_rapids_ml.classification.RandomForestClassifier - INFO - Finished training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training complete.\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "# This runs the training logic in C++ on the GPU via cuML\n",
    "print(\"Training Spark RAPIDS ML model...\")\n",
    "rf_model = rf_classifier.fit(training_data)\n",
    "print(\"Model training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e76d90bb-fcd7-441d-8240-b58706be5d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/02 13:36:02 WARN  util.SparkStringUtils: [Thread-6]: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 7:>                                                          (0 + 2) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|prediction|fraud_trx|\n",
      "+----------+---------+\n",
      "|       0.0|        0|\n",
      "|       0.0|        0|\n",
      "|       0.0|        0|\n",
      "|       0.0|        0|\n",
      "|       0.0|        1|\n",
      "+----------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "== Physical Plan ==\n",
      "GpuColumnarToRow (8)\n",
      "+- GpuProject (7)\n",
      "   +- GpuCoalesceBatches (6)\n",
      "      +- GpuArrowEvalPython (5)\n",
      "         +- GpuCoalesceBatches (4)\n",
      "            +- GpuSample (3)\n",
      "               +- GpuSort (2)\n",
      "                  +- GpuScan parquet spark_catalog.default.datalaketable (1)\n",
      "\n",
      "\n",
      "(1) GpuScan parquet spark_catalog.default.datalaketable\n",
      "Output [15]: [age#0, credit_card_balance#1, bank_account_balance#2, mortgage_balance#3, sec_bank_account_balance#4, savings_account_balance#5, sec_savings_account_balance#6, total_est_nworth#7, primary_loan_balance#8, secondary_loan_balance#9, uni_loan_balance#10, longitude#11, latitude#12, transaction_amount#13, fraud_trx#14]\n",
      "Batched: true\n",
      "Eager_IO_Prefetch: false\n",
      "Location: InMemoryFileIndex [s3a://go01-demo/warehouse/tablespace/external/hive/customer_data HB01/warehouse/tablespace/external/hive/datalaketable]\n",
      "ReadSchema: struct<age:float,credit_card_balance:float,bank_account_balance:float,mortgage_balance:float,sec_bank_account_balance:float,savings_account_balance:float,sec_savings_account_balance:float,total_est_nworth:float,primary_loan_balance:float,secondary_loan_balance:float,uni_loan_balance:float,longitude:float,latitude:float,transaction_amount:float,fraud_trx:int>\n",
      "\n",
      "(2) GpuSort\n",
      "Input [15]: [age#0, credit_card_balance#1, bank_account_balance#2, mortgage_balance#3, sec_bank_account_balance#4, savings_account_balance#5, sec_savings_account_balance#6, total_est_nworth#7, primary_loan_balance#8, secondary_loan_balance#9, uni_loan_balance#10, longitude#11, latitude#12, transaction_amount#13, fraud_trx#14]\n",
      "Arguments: [age#0 ASC NULLS FIRST, credit_card_balance#1 ASC NULLS FIRST, bank_account_balance#2 ASC NULLS FIRST, mortgage_balance#3 ASC NULLS FIRST, sec_bank_account_balance#4 ASC NULLS FIRST, savings_account_balance#5 ASC NULLS FIRST, sec_savings_account_balance#6 ASC NULLS FIRST, total_est_nworth#7 ASC NULLS FIRST, primary_loan_balance#8 ASC NULLS FIRST, secondary_loan_balance#9 ASC NULLS FIRST, uni_loan_balance#10 ASC NULLS FIRST, longitude#11 ASC NULLS FIRST, latitude#12 ASC NULLS FIRST, transaction_amount#13 ASC NULLS FIRST, fraud_trx#14 ASC NULLS FIRST], false, com.nvidia.spark.rapids.OutOfCoreSort$@5b330fe3\n",
      "\n",
      "(3) GpuSample\n",
      "Input [15]: [age#0, credit_card_balance#1, bank_account_balance#2, mortgage_balance#3, sec_bank_account_balance#4, savings_account_balance#5, sec_savings_account_balance#6, total_est_nworth#7, primary_loan_balance#8, secondary_loan_balance#9, uni_loan_balance#10, longitude#11, latitude#12, transaction_amount#13, fraud_trx#14]\n",
      "Arguments: 0.8, 1.0, false, 1234\n",
      "\n",
      "(4) GpuCoalesceBatches\n",
      "Input [15]: [age#0, credit_card_balance#1, bank_account_balance#2, mortgage_balance#3, sec_bank_account_balance#4, savings_account_balance#5, sec_savings_account_balance#6, total_est_nworth#7, primary_loan_balance#8, secondary_loan_balance#9, uni_loan_balance#10, longitude#11, latitude#12, transaction_amount#13, fraud_trx#14]\n",
      "Arguments: targetsize(1073741824)\n",
      "\n",
      "(5) GpuArrowEvalPython\n",
      "Input [15]: [age#0, credit_card_balance#1, bank_account_balance#2, mortgage_balance#3, sec_bank_account_balance#4, savings_account_balance#5, sec_savings_account_balance#6, total_est_nworth#7, primary_loan_balance#8, secondary_loan_balance#9, uni_loan_balance#10, longitude#11, latitude#12, transaction_amount#13, fraud_trx#14]\n",
      "Arguments: [predict_udf(named_struct(age, age#0, credit_card_balance, credit_card_balance#1, bank_account_balance, bank_account_balance#2, mortgage_balance, mortgage_balance#3, sec_bank_account_balance, sec_bank_account_balance#4, savings_account_balance, savings_account_balance#5, sec_savings_account_balance, sec_savings_account_balance#6, total_est_nworth, total_est_nworth#7, primary_loan_balance, primary_loan_balance#8, secondary_loan_balance, secondary_loan_balance#9, uni_loan_balance, uni_loan_balance#10, longitude, longitude#11, ... 4 more fields))], [pythonUDF0#285], 204\n",
      "\n",
      "(6) GpuCoalesceBatches\n",
      "Input [16]: [age#0, credit_card_balance#1, bank_account_balance#2, mortgage_balance#3, sec_bank_account_balance#4, savings_account_balance#5, sec_savings_account_balance#6, total_est_nworth#7, primary_loan_balance#8, secondary_loan_balance#9, uni_loan_balance#10, longitude#11, latitude#12, transaction_amount#13, fraud_trx#14, pythonUDF0#285]\n",
      "Arguments: targetsize(1073741824)\n",
      "\n",
      "(7) GpuProject\n",
      "Input [16]: [age#0, credit_card_balance#1, bank_account_balance#2, mortgage_balance#3, sec_bank_account_balance#4, savings_account_balance#5, sec_savings_account_balance#6, total_est_nworth#7, primary_loan_balance#8, secondary_loan_balance#9, uni_loan_balance#10, longitude#11, latitude#12, transaction_amount#13, fraud_trx#14, pythonUDF0#285]\n",
      "Arguments: [age#0, credit_card_balance#1, bank_account_balance#2, mortgage_balance#3, sec_bank_account_balance#4, savings_account_balance#5, sec_savings_account_balance#6, total_est_nworth#7, primary_loan_balance#8, secondary_loan_balance#9, uni_loan_balance#10, longitude#11, latitude#12, transaction_amount#13, fraud_trx#14, pythonUDF0#285.prediction AS prediction#238], true\n",
      "\n",
      "(8) GpuColumnarToRow\n",
      "Input [16]: [age#0, credit_card_balance#1, bank_account_balance#2, mortgage_balance#3, sec_bank_account_balance#4, savings_account_balance#5, sec_savings_account_balance#6, total_est_nworth#7, primary_loan_balance#8, secondary_loan_balance#9, uni_loan_balance#10, longitude#11, latitude#12, transaction_amount#13, fraud_trx#14, prediction#238]\n",
      "Arguments: false\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Predict and optimize the output\n",
    "# We drop 'probability' and 'rawPrediction' because they are VectorUDT types\n",
    "# that Spark SQL would otherwise force back to the CPU for formatting.\n",
    "predictions = rf_model.transform(test_data).drop(\"probability\", \"rawPrediction\")\n",
    "\n",
    "# Show results (This will be fully accelerated)\n",
    "predictions.select(\"prediction\", \"fraud_trx\").show(5)\n",
    "# Verify GPU Plan\n",
    "# You should see 'GpuProject' and 'GpuFilter' nodes without the VectorUDT warning\n",
    "predictions.explain(mode=\"formatted\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1ea6ef8b-ec47-4323-9cac-8551a405c999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module spark_rapids_ml.metrics.MulticlassMetrics in spark_rapids_ml.metrics:\n",
      "\n",
      "NAME\n",
      "    spark_rapids_ml.metrics.MulticlassMetrics\n",
      "\n",
      "DESCRIPTION\n",
      "    # Copyright (c) 2024, NVIDIA CORPORATION.\n",
      "    #\n",
      "    # Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "    # you may not use this file except in compliance with the License.\n",
      "    # You may obtain a copy of the License at\n",
      "    #\n",
      "    #     http://www.apache.org/licenses/LICENSE-2.0\n",
      "    #\n",
      "    # Unless required by applicable law or agreed to in writing, software\n",
      "    # distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "    # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "    # See the License for the specific language governing permissions and\n",
      "    # limitations under the License.\n",
      "    #\n",
      "\n",
      "CLASSES\n",
      "    builtins.object\n",
      "        MulticlassMetrics\n",
      "    \n",
      "    class MulticlassMetrics(builtins.object)\n",
      "     |  MulticlassMetrics(tp: Dict[float, float] = {}, fp: Dict[float, float] = {}, label: Dict[float, float] = {}, label_count: int = 0, log_loss: float = -1) -> None\n",
      "     |  \n",
      "     |  Metrics for multiclass classification.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, tp: Dict[float, float] = {}, fp: Dict[float, float] = {}, label: Dict[float, float] = {}, label_count: int = 0, log_loss: float = -1) -> None\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  accuracy(self) -> float\n",
      "     |      Returns accuracy (equals to the total number of correctly classified instances\n",
      "     |      out of the total number of instances.)\n",
      "     |  \n",
      "     |  evaluate(self, evaluator: pyspark.ml.evaluation.MulticlassClassificationEvaluator) -> float\n",
      "     |  \n",
      "     |  false_positive_rate(self, label: float) -> float\n",
      "     |      Returns false positive rate for a given label (category)\n",
      "     |  \n",
      "     |  hamming_loss(self) -> float\n",
      "     |      Returns Hamming-loss\n",
      "     |  \n",
      "     |  log_loss(self) -> float\n",
      "     |      Returns log loss\n",
      "     |  \n",
      "     |  true_positive_rate_by_label(self, label: float) -> float\n",
      "     |      Returns true positive rate for a given label (category)\n",
      "     |  \n",
      "     |  weighted_false_positive_rate(self) -> float\n",
      "     |      Returns weighted false positive rate\n",
      "     |  \n",
      "     |  weighted_fmeasure(self, beta: float = 1.0) -> float\n",
      "     |      Returns weighted averaged f1-measure\n",
      "     |  \n",
      "     |  weighted_precision(self) -> float\n",
      "     |      Returns weighted averaged precision\n",
      "     |  \n",
      "     |  weighted_recall(self) -> float\n",
      "     |      Returns weighted averaged recall (equals to precision, recall and f-measure)\n",
      "     |  \n",
      "     |  weighted_true_positive_rate(self) -> float\n",
      "     |      Returns weighted true positive rate. (equals to precision, recall and f-measure)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  SUPPORTED_MULTI_CLASS_METRIC_NAMES = ['f1', 'accuracy', 'weightedPreci...\n",
      "\n",
      "FUNCTIONS\n",
      "    log_loss(labels: numpy.ndarray, probs: numpy.ndarray, eps: float) -> float\n",
      "        # sklearn's version will not support fixed eps starting v1.5\n",
      "\n",
      "DATA\n",
      "    Dict = typing.Dict\n",
      "        A generic version of dict.\n",
      "\n",
      "FILE\n",
      "    /home/cdsw/.local/lib/python3.10/site-packages/spark_rapids_ml/metrics/MulticlassMetrics.py\n",
      "\n",
      "\n",
      "Available in metrics: None\n"
     ]
    }
   ],
   "source": [
    "import spark_rapids_ml.metrics.MulticlassMetrics as mm\n",
    "print(f\"Available in metrics: {help(mm)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ca0eb45c-c643-45a5-ac15-b69e669b4d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU-Accelerated Accuracy: 0.8999\n"
     ]
    }
   ],
   "source": [
    "accuracy = predictions.filter(\"prediction = fraud_trx\").count() / predictions.count()\n",
    "\n",
    "print(f\"GPU-Accelerated Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "686b8c1c-acf4-4442-8d8f-0c5fc07df74a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/02 14:27:33 WARN  rapids.GpuOverrides: [Thread-6]: \n",
      "! <DeserializeToObjectExec> cannot run on GPU because not all expressions can be replaced; GPU does not currently support the operator class org.apache.spark.sql.execution.DeserializeToObjectExec\n",
      "  ! <CreateExternalRow> createexternalrow(staticinvoke(class java.lang.Double, ObjectType(class java.lang.Double), valueOf, prediction#238, true, false, true), staticinvoke(class java.lang.Double, ObjectType(class java.lang.Double), valueOf, fraud_trx#1720, true, false, true), staticinvoke(class java.lang.Double, ObjectType(class java.lang.Double), valueOf, 1.0#1721, true, false, true), StructField(prediction,DoubleType,true), StructField(fraud_trx,DoubleType,true), StructField(1.0,DoubleType,false)) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.CreateExternalRow\n",
      "    !Expression <StaticInvoke> staticinvoke(class java.lang.Double, ObjectType(class java.lang.Double), valueOf, prediction#238, true, false, true) cannot run on GPU because StaticInvoke of java.lang.Double is not supported on GPU\n",
      "      @Expression <AttributeReference> prediction#238 could run on GPU\n",
      "    !Expression <StaticInvoke> staticinvoke(class java.lang.Double, ObjectType(class java.lang.Double), valueOf, fraud_trx#1720, true, false, true) cannot run on GPU because StaticInvoke of java.lang.Double is not supported on GPU\n",
      "      @Expression <AttributeReference> fraud_trx#1720 could run on GPU\n",
      "    !Expression <StaticInvoke> staticinvoke(class java.lang.Double, ObjectType(class java.lang.Double), valueOf, 1.0#1721, true, false, true) cannot run on GPU because StaticInvoke of java.lang.Double is not supported on GPU\n",
      "      @Expression <AttributeReference> 1.0#1721 could run on GPU\n",
      "  !Expression <AttributeReference> obj#1725 cannot run on GPU because expression AttributeReference obj#1725 produces an unsupported type ObjectType(interface org.apache.spark.sql.Row)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8999\n"
     ]
    }
   ],
   "source": [
    "#from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from spark_rapids_ml.metrics.MulticlassMetrics import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"fraud_trx\", \n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"accuracy\"\n",
    ")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605ca3e1-7009-4c22-af36-c6c8035c954a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa3b6fa-ab80-4092-8970-0d38182549e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
